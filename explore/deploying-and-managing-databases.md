# Deploying and Managing Databases (and other stateful components)

Deploying and managing databases is a very tricky business. I used to do it a year or ago along with my super duper team. So I know some stuff, because we did it for around 2 years

Some of the interesting issues we noticed and solved was just mind blowing for me at the time and I learned quite a bit form my team and I still have a lot to learn in the field of databases, distributed databases and deploying and managing them

As part of this topic, I wanted to recall and also think about what are some important things to keep in mind when deploying and managing databases

I think the first key thing is to understand the context. In my case, when me and my team were working with databases and managing them for our client, we were managing many instances of databases servers and the databases were mostly PostgreSQL and Redis and some of them were MongoDB, like around 2 of them MongoDB, and then one MySQL database which was used by a legacy service, and that's it I think. There were other stateful components too though, like RabbitMQ which was using disk, and then there was Consul which was stateful too. I think it's important to understand stateful components in general and be careful with them as they are usually hard to deploy and manage, atleast that's the experience I had and that's what I hear from many people. But, well, I think with practice and learning and experience, things should get easier to solve, even if the problem is complex. We also had Prometheus which was stateful. Whenever I say stateful, I mean that it maintained state with the help of a disk, unlike services which don't maintain state and don't use disk so no disks are attached to those services. I don't know what other methods are there to maintain state, I mean, a service could delegate the state maintenance to some other service too, for example, services which serve APIs which need state simply use databases or other external services to maintain the state by communicating with them and delegating the problem of state maintenance to them. Generally I have seen that in such cases, since the service delegates state maintanence, it's called stateless as it doesn't store any state by itself, but instead delegates it to other services in case it needs it. It's funny because however such APIs are also having some state but through delegation to another service, anyways.

I have also seen state being maintained in-memory by microservices, for example, microservices caching the IP address of the database after resolving the database DNS and then when database IP address changes but DNS does not, the microservice still uses cache and fails to conenct to the database and does not even retry to resolve the DNS assuming IP address cache might be invalid. These are some really annoying cases of caching. I think it's important to check about possibility of cache invalidation when doing caching. Not sure how important libraries missed these and I have seen some microservices in our previous client's system which did such caching because the internal libraries did it, I think it was open source libraries only, but I never looked into the code of all the microservices that had such IP caching issues

Now, let's get back to deploying and managing databases and other stateful components too ;)

Now, before deploying and managing databases, I think it's key to understand the database itself. When me and my team were working on managing databases, I don't think everyone in my team knew everything or many things about all the databases we were working with. For example, we worked a lot with Postgres and Redis and many knew mostly about that, compared to other databases, which were less in numbers, and it kind of made sense. Later we also learned about RabbitMQ message queue, a stateful component, when we had to work on it

Even with Postgres and Redis, I don't think everyone knew a lot about both of them. I think usually people were obsessed with Postgres, and knew Redis just a bit, but yeah, that's just my hunch based on team experience and talks with the team. We also had tons of issues with our Postgres which inherently caused us to talk a lot about it

We were deploying all of our stateful components on Kubernetes. At times we deployed the components on VMs, for example we deployed Kafka to VMs and sometimes even Postgres to VMs, when teams wanted to migrate from Kubernetes to good old VMs

The reason for using Kubernetes was simple I guess - we were using Kubernetes to run stateless workloads, why not stateful workloads too? How is running in Kubernetes different from running in VMs? In fact it should run just fine in Kubernetes, if it is not, then it's our mistake for not running properly on Kubernetes but not Kubernetes issue. At least that's the idea the team kind of had. I didn't know better then. But later I realized that running stateful workloads on Kubernetes is no piece of cake and that many people have talked about this online and in the world and it's a pretty common and hot topic. It's still prevalent today

I think running stateful components in general is a hot topic, and running it on top of Kubernetes is just hotter probably because Kubernetes is a buzz word and a hot topic. Not to mention, stateful workloads can also run on top of other orchestration platforms like Nomad and what not. I have actually seen talks by some folks who have run stateful workloads on Nomad and using Portworx https://portworx.com/ for storage solutions

Portworx actually seems to have some interesting projects in the space of storage

https://portworx.com/

https://portworx.com/products/open-source/ - open source projects, seems interesting!

and some groups

https://github.com/cncf/tag-storage

https://github.com/kubernetes/community/tree/master/sig-storage

Oops, I was looking at more projects and stuff and got lost in the Internet of links. Lol. Now, back to database stuff!

So, let's see why running and managing databases might be hard and especially hard on Kubernetes. These are just my thoughts based on experience

Running and managing databases might be hard if one has no idea about the database. Duh. I mean, let's say I'm an operator / system administrator (sysadmin) / databases administrator (DBA) taking care of running and managing say Redis database. If I don't even know about Redis, then it's obviously gonna be hard. And there's more to this, let's dig deep

When I say "no idea about the database" I mean "no idea to run and manage a database" / "no idea about the database's world and it's intricacies and it's working". I think it's key to understand the database and a lot more before one gets into deploying and managing it. I think it goes without saying, but I guess it's best to mention this explicitly though one could be like "You don't say" :P So, what does one need to know about the database to deploy and manage it, my take? I would say the person needs to know almost everything about it. Yes. Almost everything. I'm not at all kidding. I mean, let's say you have a microservice, that you built, and then if I ask someone else, who has no idea about the microservice and hasn't contributed to it / looked at the code, to deploy and run it, and also manage it, do you think they can do it? Of course not. What would they need to know to work with your microservice? Almost everything. Same goes for your databases. At least that's my take. Let's take a look at what someone would have to do to become a kind of an expert to actually deploy and manage databases

Let's say I want to run and manage Redis database, just for an example. What would I need to know? Well, for starters, apart from knowing, I should have actually tried Redis database as a user. It's awesome if I have tried Redis database as a user in a production environment where I run a microservice or some serivce which uses the Redis database and where I write code to connect to the Redis database using some Redis client library and also manage the microservice, assuming Redis is managed by some SaaS company or by some platform team or some-name-that-i-dont-care team within the company. This kind of experience gives me an idea of how a user uses Redis, how a developer uses Redis and codes for it. How Redis works from the view of a simple user / developer who doesn't / may not know too much. What kind of operations the developer / user performs when interacting with the Redis through code or other ways like CLI client or GUI client. What kind of annoyances they faced as a user / developer. What kind of experience they had? Where did they feel they need more control, or less control and wanted the SaaS company or the team to take care of things. And there's more and more and more. I mean, as a user / developer, you will get to know for what kind of use case you chose Redis and why. And also know stuff like what kind of use cases Redis can help with, and how it helps and why someone would choose Redis

I think it's important for operators to be some sort of experts. And that includes undertanding users and seeing from the lens of a user and also understanding what the database provides, what are the features it has. What is the database meant to do? What was it built for? What can it be used for and what it can't be used for and what it should NOT be used for, like, anti-patterns, where say, maybe the database performs bad for use cases it was NOT built for

I think when it comes to performance, it's important to understand the conditions under which a system can be performant / perform well, and conditions under which it can NOT perform well. Also, I have seen this interesting analogy where - it's good to design the system to be performant for the situations that will happen 99% of the times instead of optimizing performance for the situations that will happen 1% of the time. So, sometimes, one could also not solve the problem of performance and any other optimization for the 1% of situations, and instead focus on 99% of the situations, or they could solve for both and cover 100%, but yeah, each choice has tradeoffs. This thing is something I learned from the Raft research paper where they solve with understandability, simplicity in mind and also think about how intuitive the solution is, and they ensure the Raft system is performant in the important and critical situations and normal state of the system which occurs 99% of the time, and not for the 1% as it more of an unusual thing and not the normal state of the system, as you can see, it happens only 1% of the time.

Now, coming to other things that operators would be better off knowing are - understanding the internal details of the database and how to interact with it like an advanced user to extract whatever information they need. In my time managing kubernetes clusters and databases on top of them for our client, I remember so many extraordinary and brilliant developers who knew a lot about their systems and who knew exactly what they were doing and were very experienced folks who had experience with tons of traps in the past, traps they had fallen into and learned from. Some things just comes from experience if not from knowledge gained while learning from sources like books and what not. I think it was really cool for developers to know every single detail about the database they were using. I think it's kind of important for any advanced / senior developer running services in production using databases to know such intricate details so that they design and architect their system with those internal intricate details in mind. For example, when working with Postgres, if the developers are using indexes then it's important that they know about it, because it's inevitable that at some point they might face issues or some problems with respect to indexes or any traps that comes with indexes, or say absence of indexes - some performance issues, so it's obvious that developers need to know their databases to a good extent, and it's cooler if they know how to even manage when issues pop up. I think it's cool when developers can handle their complete system by themselves as a team by learning every component in the system to be able to run and manage it

I remember once how Redis was having performance issues and me and my team were struggling to find the root cause and the developers kind of helped find the root cause later. Many a times, developers knew a lot more than our team, sometimes our team knew some more stuff

Also, while managing databases, it's key to understand what kind of services it's serving and what it's supposed to do / supposed to be doing. For example, databases these days can do tons of stuff, like, in Postgres, I think there's an extension to work with Spatial Data using PostGIS extension. So, if operators know what the database is doing - let's say it's working with spatial data along with PostGIS extension, then it's easy for them to know what it should doing and what it should not be doing and what kind of issues can the database operations face etc

Issues brings us to the point about monitoring and alerting for database and other stateful components. This is very very important similar to how it is important to do monitoring and alerting for any system, like your microservices and what not. To monitor the database, it is important to understand what kind of metrics you want to monitor and for what situations you want alerts. Monitoring and alerting is a very big topic actually. And a topic by itself. I just want to touch upon some basic stuff that I recall that we did from my time while managing databases and other stateful components. There are some standard generic things that one can monitor - for example, check the resource usage at machine and process level, for example - CPU, RAM, and things like Network, Disk

You may want alerts when the CPU spikes or keeps spiking and going higher up, say you might want an alert when CPU goes above 60% or 80%, or whatever value makes sense for your system. It's also important to do the same for other resource usage too. This will help you to add more resources in case more is needed, or check why more is being used when it should actually use lesser or just the right amount if the required resources for the given component is already available

I remember we had one of these disk alerts where we would check the current rate of increase of disk usage and extrapolate and guess the future value assuming the rate is consitent, basically saying the value will increase at the same rate linearly, though it could be exponential or just a sudden spike too, so, with this extrapolation, we would send an alert saying something similar to "Disk usage rate is xyz MB/s and it might get full in 2 hrs" and also another alert where we say "Disk usage rate is xyz MB/s and it might get full in 24 hrs". This is to give us enough time to check why the disk usage is increasing and maybe increase the disk size if the increase is expected. Sometimes we used to get funny alerts when the total disk usage was only some MBs, then we had to put extra detail in the alert saying what's the current disk usage apart from showing the rate of increase in disk usage, these alerts were more like false alerts which happened due to short spikes of sorts.

I think there is a lot of good content around monitoring and alerting out there and how to do them best to avoid burn out and toil for teams. For example, avoiding spamming of alerts and avoiding non-actionable alerts and of course avoiding false alerts, and what not

In our case, we did have a spam of alerts, to the point where some of us sometimes ignored it, especially ignored the staging environment alerts, somtimes ignoring the false alerts in production too. Our then tech lead used to tell how we need to keep looking into the alerts and also reduce the noise. At some point we did reduce the noise by removing alerts we didn't care about and we didn't take action on. So, at some point, the idea was - alerts should be received only when we need to take an action, which made a lot of sense! :)

We used Prometheus for our monitoring system and Grafana for visualizing the metrics. We used a lot of prometheus exporters which exported metrics from a lot of the databases and stateful components we were using

We had a single centra Grafana deployed to a kubernetes cluster that our client was already maintaining and we added all the prometheus servers from all the kubernetes clusters in all the different environments into the central Grafana as data sources and added dashboards using community dashboards and also customizing them

One of the customizations we did was - adding a drop down for choosing the prometheus data source. So, using a single Grafana dashboard, we could look at the data for all the Kubernetes clusters where our stateful workloads were running along with Prometheus, we had something similar for our stateless workloads too, for some system level / platform level metrics, apart from service teams using InfluxDB for application level metrics including custom ones / business metrics and also using systems like New Relic to monitoring Application Performance

Metrics can be at different levels, for example -
- CPU, RAM, Disk, Network - this is very generic and at system level
- Process specific - for example, when running Redis, if you want to show Redis database size, you can have `DBSIZE` command's value as one of the metrics, it will give the number of keys in the database. https://redis.io/commands/dbsize . There are many such process specific metrics. As part of process specific metrics, one can also consider runtime metrics of the process. For example, when running a golang program, apart from the program providing metrics, golang has metrics of it's own for it's golang runtime, for example, number of golang routines running in the process and there's more, like maybe one can get garbage collection details etc
- Business specific - This is also kind of process specific. For example, when running microservices, let's say your microservice gets requests from 100 authenticated clients, one metric can show how many authenticated clients are communicating with the microservice. This can be an important metric named as "Number of active users" for the business to show and take any decisions based on such metrics

Each metric can be obtained in different ways. It's easy to get CPU, RAM, Disk and Network metrics at machine level by just knowing about the OS the machine runs on and if the OS can help with that data. But process specific and business metrics are very specific. One needs special programs that understand the process and business to capture those metrics. Sometimes baked into the process / application itself

If a database comes with it's own monitoring and alerting system / tool for monitoring the database and alerting when something goes wrong, that's pretty cool! Or there must atleast be hooks to the database system to help us get any internal data regarding the database's working. For example, Redis has `LATENCY DOCTOR` command among other `LATENCY` commands to help with latency issues - https://redis.io/topics/latency-monitor . There are also so many other tools that Redis provides to observe and monitor Redis which is pretty cool. I think most modern databases would be doing something to help with some sort of monitoring and alerting for the database

The best thing is - when the database can self heal in the event of issues. This is also a feature that modern databases are trying to bake into their database systems, or at least provide tools for it

The key thing is - I have heard this statement where someone said something along the lines of - "when monitoring, we know what we are looking for, we know what questions we want answers for, when doing observability, we don't know the questions we want to answer, we will ask some of those questions when the issue happens and we need to know answers, for which we need to capture data". But...how does one capture data to answer a question that we don't know about? how does capture data to answer a question which we will ask only in the future, which we will know only then? So, it's more along the lines of - let's say you capture some metrics, later when an issue happens, if you ask some questions that those metrics can't answer, then what use are those metrics? Right? This episode from Tech Lead Journal has a lot of interesting details about this -

https://techleadjournal.dev/episodes/36/

I think that to observe the database system and other surrounding systems completely, or say all your systems completely, you would have to capture a lot of data and put it all in one single place and then be able to ask questions and find answers there. I think logging is one thing that I have heard people talk about. I mean, I know cases where people correlate logs with the help of IDs unique to the whole system, IDs which recognize a request or operation in a distributed system - so, more like helping with a distributed tracing kind of situation. Shipping logs to a central server and searching them etc is a tedious in my eye, but me and my team have found some issues in systems using such logs, for example in GCP we did it quite a lot for some kinds of issues

The key is - it's important to understand the database's internal details, what it's being used for and the internal details of that and any database features that help with that and then know what all issues can occur. But knowing all the issues that can occur might be a pretty difficult thing, but one can learn by checking history - what kind of issues have other teams faced, other teams in the same company, other teams in the world, if their story is online and public.

This is where Root Cause Analysis (RCA) documents come in very handy. Learn from failures, others failures, and your own failures. Learn what happened, what caused the issue, how it was fixed. Learn the sequence of events, learn how the issue was detected or found (alerts? customer support ticket from customer? social media rants?)

Runbook for debugging - manual

Automate - automate runbook - checking issues / checking stats / automatic diagnostics and debugging and recommending solutions

Runbook for operating (install, upgrade, resize disk, add shard, remove shard) - manual

Automating operations - using automatic operators, for example Kubernetes operators to run and manage databases and other stateful components

---

We used alertmanager for alerts on slack and pagerduty

---

Understanding what the database is being used for is key like I had already mentioned. For example, when we were maintaing a Redis for our clients, the Redis was actually used as a cache for the API results from Google Maps API. Why? Google Maps API is a paid product and costs a lot and the cost is based on API call count, and so, if there's response that's cached and valid, then it can be reused instead of hitting the API again which will cost the client more. So for this reason, Redis was being used. Now, usually cache is considered to be something as - "it's okay if we lose it, we can build the cache again", at least that's what I thought. But hey, guess what, in this case, and in many cases, losing a cache can be pretty costly and catostropic, how? Well, in the case of caching Google Maps API response, if the cache is lost due to some reason, say the Redis goes down and it was all in-memory and it had no persistence, and all cache is lost, it has to be built up again, and all that cached response is gone, so more API calls will be made now, which could have otherwise been reduced if the Redis cache didn't go down, and more API calls means more money. So it costs a lot of money in this case, to lose the cache. Also, another obvious thing that will happen is the operation will slow down a bit, because usually a cache helps to speed things up compared to the normal operation, in case the result is cached. But when the cache is gone, operations are expected to be a bit slower. So, losing a cache can be costly regardless, it can slow down the system, for example response times can go up if cache is lost. Slow response means the system is slow, which can have bad effects on the business in general, for various technical and non-technical reasons. One technical reason is - strict timeouts, this can chuck the slow responses. So, some teams can't really afford to slower their API's response time, the 99th percentile especially

So, what do we do? We need to ensure the Redis is HA in such a case or maybe use Redis Cluster which has both sharding and HA features. Also, if possible, persistence can be enabled in Redis given it doesn't affect the speed of the Redis to act as a cache, as caches are supposed to be fast. Or atleast try taking backups if persistence is not enabled, in order to restore data from backup in case of disasters, so that cache ain't gone. But yeah, these are just ideas based on what I thought, I'm not sure what people generally follow based on the context. I'll be reading more caches soon, along with cache invalidation etc, so yeah, not sure if backup of cache is cool or not, but surely losing cache may not be a very cool or nice thing and can cause craziness and chaos in the system at times

---

Status pages and dashboards for customers / public

For example

https://status.aws.amazon.com/

https://carl.gg/status

https://www.githubstatus.com/

If it's for the public, then one has to use Social Media accounts too for sharing status updates, this is popular trend and customers might expect this from other services too

Example - https://twitter.com/githubstatus

Also, notifying about status can also help. GitHub Status page has feature to subscribe to GitHub Status updates - through email, phone text (SMS), Webhook (!!!), Atom / RSS Feed, and also Social Media - by following their twitter profile or simply checking it out like I mentioned
